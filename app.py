import streamlit as st
import onnxruntime as ort
import numpy as np
import cv2
from PIL import Image
import os

st.set_page_config(page_title="äºŒæ¬¡å…ƒè½¬æ¢å™¨", page_icon="ğŸ¨")

# ä½ çš„æ¨¡å‹æ–‡ä»¶å (ç¡®ä¿GitHubä¸Šä¹Ÿæ˜¯è¿™ä¸ªåå­—)
MODEL_FILE = "2_4_paprika.onnx"

def process_image(image, size=512):
    """
    é¢„å¤„ç†ï¼š
    1. è°ƒæ•´å¤§å°
    2. å½’ä¸€åŒ–
    3. å…³é”®ä¿®æ”¹ï¼šHWC -> CHW (æŠŠé€šé“ç§»åˆ°å‰é¢)
    """
    image = np.array(image.convert('RGB'))
    # æ³¨æ„ï¼šè¿™ä¸ªæ¨¡å‹éœ€è¦ RGB æ ¼å¼ï¼Œä¸è¦è½¬ BGR
    # image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) 
    
    h, w = image.shape[:2]
    
    # è°ƒæ•´å¤§å°é€»è¾‘
    scale = size / max(h, w)
    new_h, new_w = int(h * scale), int(w * scale)
    new_h = new_h - (new_h % 32)
    new_w = new_w - (new_w % 32)
    
    if new_h == 0 or new_w == 0: return None
    
    image = cv2.resize(image, (new_w, new_h))
    image = image.astype(np.float32)
    image = image / 127.5 - 1.0
    
    # --- ä¿®å¤æ ¸å¿ƒï¼šç»´åº¦ç½®æ¢ ---
    # åŸå›¾æ˜¯ (High, Width, Channel)ï¼Œæ¨¡å‹è¦ (Channel, High, Width)
    image = image.transpose(2, 0, 1) 
    
    image = np.expand_dims(image, axis=0)
    return image

def run_inference(image_pil):
    if not os.path.exists(MODEL_FILE):
        st.error(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {MODEL_FILE}ï¼Œè¯·æ£€æŸ¥GitHubä¸Šä¼ æ˜¯å¦æˆåŠŸã€‚")
        st.stop()

    try:
        # ç¦ç”¨ä¸€äº›ä¼˜åŒ–ä»¥æé«˜å…¼å®¹æ€§
        sess_options = ort.SessionOptions()
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_BASIC
        session = ort.InferenceSession(MODEL_FILE, sess_options)
    except Exception as e:
        st.error(f"âŒ æ¨¡å‹åŠ è½½å‡ºé”™: {e}")
        st.stop()

    x_name = session.get_inputs()[0].name
    y_name = session.get_outputs()[0].name
    
    img_input = process_image(image_pil)
    if img_input is None: return None
    
    # æ¨ç†
    fake_img = session.run([y_name], {x_name: img_input})[0]
    
    # åå¤„ç†ï¼šæŠŠç»´åº¦æ¢å›æ¥
    fake_img = fake_img.squeeze() # å»æ‰ batch ç»´åº¦
    # (Channel, High, Width) -> (High, Width, Channel)
    fake_img = fake_img.transpose(1, 2, 0) 
    
    fake_img = (fake_img + 1.0) * 127.5
    fake_img = np.clip(fake_img, 0, 255).astype(np.uint8)
    
    return Image.fromarray(fake_img)

# --- ä¸»é¡µé¢ ---
st.title("ğŸ¨ AI åŠ¨æ¼«ç»˜å›¾")
st.write("å·²åŠ è½½ FacePaint æ²¹ç”»é£æ ¼æ¨¡å‹")

uploaded_file = st.file_uploader("è¯·ä¸Šä¼ å›¾ç‰‡", type=['jpg', 'png', 'jpeg'])

if uploaded_file:
    original_image = Image.open(uploaded_file)
    st.image(original_image, caption="åŸå›¾", use_column_width=True)
    
    if st.button("âš¡ ç«‹å³è½¬æ¢", type="primary"):
        with st.spinner("AI æ­£åœ¨ç»˜å›¾ï¼Œè¯·ç¨å€™..."):
            try:
                anime_image = run_inference(original_image)
                if anime_image:
                    st.image(anime_image, caption="ç”Ÿæˆç»“æœ", use_column_width=True)
            except Exception as e:
                st.error(f"è¿è¡Œå‡ºé”™: {e}")
